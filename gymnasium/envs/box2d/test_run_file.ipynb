{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "### First Attempt at training AI (it was a little dumb and constantly used the booster causing it to never land)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_lunar\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(observation)  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d665407b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10083/1973446800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create and wrap the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DummyVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "### First attempt at also printing the score after each episode\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# Create and wrap the environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_lunar\")\n",
    "# evaluate_policy(model, env, n_eval_episodes=1, render=True)\n",
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        score+=rewards\n",
    "    print(f\"Episode: {episode}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78ff6454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 13348\n",
      "observations: -0.01 +1.42 -0.65 +0.26 +0.01 +0.10 +0.00 +0.00\n",
      "step 0 total_reward +7.56\n",
      "====================================\n",
      "Process ID: 13348\n",
      "observations: -0.13 +1.41 -0.58 -0.27 -0.11 -0.15 +0.00 +0.00\n",
      "step 20 total_reward +287.08\n",
      "====================================\n",
      "Process ID: 13348\n",
      "observations: -0.25 +1.16 -0.59 -0.81 -0.24 -0.11 +0.00 +0.00\n",
      "step 40 total_reward +377.79\n",
      "====================================\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"test_multiple_landers.py\", line 894, in demo_heuristic_lander\n",
      "    s, r, terminated, truncated, info = step_api_compatibility(env.step(a), True)\n",
      "  File \"test_multiple_landers.py\", line 650, in step\n",
      "    self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
      "  File \"test_multiple_landers.py\", line 82, in EndContact\n",
      "    if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
      "AttributeError: 'LunarLander' object has no attribute 'legs'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"test_multiple_landers.py\", line 949, in <module>\n",
      "    pool.map(demo_heuristic_lander, args_list)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 268, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "AttributeError: 'LunarLander' object has no attribute 'legs'\n"
     ]
    }
   ],
   "source": [
    "!python test_multiple_landers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fb86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File executed!\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"test_train.py\", line 950, in <module>\r\n",
      "    run_trained_model(env)\r\n",
      "  File \"test_train.py\", line 927, in run_trained_model\r\n",
      "    model = A2C.load(\"A2C_Model\", env=env)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/base_class.py\", line 683, in load\r\n",
      "    print_system_info=print_system_info,\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\", line 390, in load_from_zip_file\r\n",
      "    load_path = open_path(load_path, \"r\", verbose=verbose, suffix=\"zip\")\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/functools.py\", line 840, in wrapper\r\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\", line 234, in open_path_str\r\n",
      "    return open_path(pathlib.Path(path), mode, verbose, suffix)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/functools.py\", line 840, in wrapper\r\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\", line 286, in open_path_pathlib\r\n",
      "    return open_path(path, mode, verbose, suffix)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/functools.py\", line 840, in wrapper\r\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\", line 266, in open_path_pathlib\r\n",
      "    raise error\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\", line 258, in open_path_pathlib\r\n",
      "    path = path.open(\"rb\")\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/pathlib.py\", line 1208, in open\r\n",
      "    opener=self._opener)\r\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/pathlib.py\", line 1063, in _opener\r\n",
      "    return self._accessor.open(self, flags, mode)\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'A2C_Model.zip'\r\n"
     ]
    }
   ],
   "source": [
    "!python \"test_train.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
