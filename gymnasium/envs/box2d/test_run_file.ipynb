{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "### First Attempt at training AI (it was a little dumb and constantly used the booster causing it to never land)\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_lunar\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(observation)  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d665407b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10083/1973446800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create and wrap the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DummyVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "### First attempt at also printing the score after each episode\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# Create and wrap the environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_lunar\")\n",
    "# evaluate_policy(model, env, n_eval_episodes=1, render=True)\n",
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        score+=rewards\n",
    "    print(f\"Episode: {episode}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78ff6454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: +0.01 +1.42 +0.33 +0.16 -0.01 -0.03 +0.00 +0.00\n",
      "step 0 total_reward +1.64\n",
      "observations: +0.01 +1.39 +0.39 -0.49 -0.01 -0.05 +0.00 +0.00\n",
      "step 0 total_reward -0.60\n",
      "observations: -0.00 +1.40 -0.24 -0.32 +0.00 +0.02 +0.00 +0.00\n",
      "step 0 total_reward -0.88\n",
      "observations: -0.00 +1.41 -0.05 +0.07 +0.00 +0.01 +0.00 +0.00\n",
      "step 0 total_reward +1.96\n",
      "observations: +0.08 +1.04 +0.34 -1.02 +0.11 +0.14 +0.00 +0.00\n",
      "step 20 total_reward -22.04\n",
      "observations: +0.07 +1.36 +0.29 -0.38 +0.12 +0.13 +0.00 +0.00\n",
      "step 20 total_reward -14.92\n",
      "observations: -0.05 +1.13 -0.21 -0.85 -0.08 -0.09 +0.00 +0.00\n",
      "step 20 total_reward -30.26\n",
      "observations: -0.01 +1.32 -0.05 -0.47 +0.01 +0.01 +0.00 +0.00\n",
      "step 20 total_reward -28.05\n",
      "observations: -0.02 +0.98 -0.04 -1.00 +0.00 -0.03 +0.00 +0.00\n",
      "step 40 total_reward -46.82\n",
      "observations: +0.13 +1.07 +0.30 -0.91 +0.22 +0.08 +0.00 +0.00\n",
      "step 40 total_reward -44.98\n",
      "observations: -0.09 +0.69 -0.14 -0.90 -0.18 -0.08 +0.00 +0.00\n",
      "step 40 total_reward -3.35\n",
      "observations: +0.12 +0.62 +0.10 -0.76 +0.22 +0.10 +0.00 +0.00\n",
      "step 40 total_reward +34.60\n",
      "observations: -0.03 +0.58 -0.01 -0.67 -0.04 -0.02 +0.00 +0.00\n",
      "step 60 total_reward +17.26\n",
      "observations: +0.17 +0.65 +0.06 -0.84 +0.28 +0.05 +0.00 +0.00\n",
      "step 60 total_reward -3.23\n",
      "observations: -0.08 +0.37 +0.19 -0.54 -0.18 +0.08 +0.00 +0.00\n",
      "step 60 total_reward +56.28\n",
      "observations: +0.14 +0.33 -0.27 -0.62 +0.26 -0.11 +0.00 +0.00\n",
      "step 80 total_reward +41.74\n",
      "observations: +0.10 +0.33 -0.24 -0.52 +0.19 -0.11 +0.00 +0.00\n",
      "step 60 total_reward +80.23\n",
      "observations: -0.02 +0.34 +0.05 -0.45 -0.04 +0.02 +0.00 +0.00\n",
      "step 80 total_reward +58.73\n",
      "observations: +0.08 +0.11 -0.42 -0.38 +0.16 -0.13 +0.00 +0.00\n",
      "step 100 total_reward +79.37\n",
      "observations: -0.02 +0.18 +0.30 -0.27 -0.07 +0.12 +0.00 +0.00\n",
      "step 80 total_reward +99.03\n",
      "observations: +0.04 +0.17 -0.35 -0.28 +0.04 -0.17 +0.00 +0.00\n",
      "step 80 total_reward +119.91\n",
      "observations: -0.01 +0.18 -0.00 -0.29 -0.04 -0.01 +0.00 +0.00\n",
      "step 100 total_reward +87.26\n",
      "observations: +0.04 +0.08 +0.32 -0.17 +0.11 +0.15 +0.00 +0.00\n",
      "step 100 total_reward +103.91\n",
      "observations: -0.02 +0.00 -0.52 -0.12 -0.00 -0.22 +0.00 +0.00\n",
      "step 120 total_reward +104.84\n",
      "observations: -0.03 +0.07 -0.34 -0.21 -0.13 -0.13 +0.00 +0.00\n",
      "step 100 total_reward +121.31\n",
      "observations: -0.00 +0.08 +0.09 -0.16 -0.00 +0.07 +0.00 +0.00\n",
      "step 120 total_reward +106.81\n",
      "observations: -0.11 -0.00 -0.40 +0.00 +0.00 -0.00 +0.00 +1.00\n",
      "step 140 total_reward +117.95\n",
      "observations: +0.09 +0.03 +0.17 -0.08 +0.19 +0.04 +0.00 +0.00\n",
      "step 120 total_reward +107.68\n",
      "observations: -0.09 +0.02 -0.22 -0.09 -0.18 +0.37 +1.00 +0.00\n",
      "step 120 total_reward +137.24\n",
      "observations: +0.01 +0.01 +0.08 -0.14 +0.06 +0.02 +0.00 +0.00\n",
      "step 140 total_reward +106.28\n",
      "observations: -0.17 -0.01 -0.20 -0.01 +0.06 +0.04 +0.00 +0.00\n",
      "step 160 total_reward +115.58\n",
      "observations: +0.11 -0.01 +0.01 +0.04 -0.04 +0.14 +1.00 +1.00\n",
      "step 140 total_reward +155.30\n",
      "observations: -0.11 -0.00 +0.00 +0.02 +0.02 -0.07 +1.00 +0.00\n",
      "step 140 total_reward +171.41\n",
      "observations: +0.02 -0.00 +0.01 -0.00 -0.00 -0.00 +1.00 +1.00\n",
      "step 160 total_reward +144.41\n",
      "observations: +0.11 -0.00 -0.00 +0.00 -0.00 +0.00 +0.00 +1.00\n",
      "step 160 total_reward +152.81\n",
      "observations: -0.11 -0.00 -0.00 -0.00 +0.00 +0.00 +1.00 +0.00\n",
      "step 160 total_reward +175.78\n",
      "observations: -0.19 -0.01 +0.01 +0.00 +0.08 -0.00 +0.00 +0.00\n",
      "step 180 total_reward +130.93\n",
      "observations: +0.03 -0.00 +0.00 +0.00 -0.00 -0.00 +1.00 +1.00\n",
      "step 180 total_reward +145.60\n",
      "observations: +0.11 -0.00 +0.00 +0.00 -0.00 +0.00 +0.00 +1.00\n",
      "step 176 total_reward +252.81\n",
      "observations: +0.03 -0.00 +0.00 +0.00 -0.00 +0.00 +1.00 +1.00\n",
      "step 186 total_reward +245.60\n",
      "observations: -0.11 -0.00 +0.00 +0.00 +0.00 +0.00 +1.00 +0.00\n",
      "step 176 total_reward +275.78\n",
      "observations: -0.18 -0.01 +0.02 +0.00 +0.08 -0.01 +0.00 +0.00\n",
      "step 200 total_reward +129.23\n",
      "observations: -0.18 -0.01 +0.02 +0.00 +0.07 -0.00 +0.00 +1.00\n",
      "step 220 total_reward +140.27\n",
      "observations: -0.18 -0.01 +0.00 +0.00 +0.07 -0.00 +0.00 +1.00\n",
      "step 240 total_reward +142.19\n",
      "observations: -0.18 -0.01 +0.00 +0.00 +0.07 +0.00 +0.00 +1.00\n",
      "step 249 total_reward +242.19\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"test_multiple_landers.py\", line 866, in demo_heuristic_lander\n",
      "    s, info = env.reset(seed=seed)\n",
      "AttributeError: 'NoneType' object has no attribute 'reset'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"test_multiple_landers.py\", line 915, in <module>\n",
      "    pool.map(demo_heuristic_lander, [env, None, True] * NUM_PROCESSES)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 268, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "AttributeError: 'NoneType' object has no attribute 'reset'\n"
     ]
    }
   ],
   "source": [
    "!python test_multiple_landers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2fb86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File executed!\n",
      "/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/policies.py:260: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  observation = np.array(observation)\n",
      "Traceback (most recent call last):\n",
      "  File \"test_train.py\", line 950, in <module>\n",
      "    run_trained_model()\n",
      "  File \"test_train.py\", line 941, in run_trained_model\n",
      "    action = model.predict(obs)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/base_class.py\", line 555, in predict\n",
      "    return self.policy.predict(observation, state, episode_start, deterministic)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/policies.py\", line 346, in predict\n",
      "    observation, vectorized_env = self.obs_to_tensor(observation)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/policies.py\", line 264, in obs_to_tensor\n",
      "    vectorized_env = is_vectorized_observation(observation, self.observation_space)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/utils.py\", line 399, in is_vectorized_observation\n",
      "    return is_vec_obs_func(observation, observation_space)\n",
      "  File \"/home/psriley/miniconda3/envs/tf15/lib/python3.7/site-packages/stable_baselines3/common/utils.py\", line 269, in is_vectorized_box_observation\n",
      "    + \"or (n_env, {}) for the observation shape.\".format(\", \".join(map(str, observation_space.shape)))\n",
      "ValueError: Error: Unexpected observation shape (2,) for Box environment, please use (8,) or (n_env, 8) for the observation shape.\n"
     ]
    }
   ],
   "source": [
    "!python test_train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
